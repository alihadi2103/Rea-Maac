alg_params:
	episodic: False
	replay: True
	policy_lrate: 0.0001
	critic_lrate: 0.0001
	mixer: False
	continuous: True
	deterministic: True
	eval_freq: 1000
	hid_size: 64
	critic_hid_size: 64
	target: True
	sa_sizes: [(2, 1), (2, 1), (2, 1), (2, 1)]
	layernorm: True
	hid_activation: relu
	norm_input: True
	norm_in: False
	save_model_freq: 2
	train_episodes_num: 2
	max_steps: 30
	agent_type: rnn
	shared_params: False
	target_lr: 0.001
	behaviour_update_freq: 10
	policy_update_epochs: 4
	value_update_epochs: 5
	batch_size: 12
	replay_warmup: 10
	init_type: normal
	agent_id: False
	attend_heads: 2
	init_std: 0.1
	replay_buffer_size: 15
	value_lrate: 0.0001
	entr: False
	reward_normalisation: False
	gamma: 0.99
	setpoint_change_freq: 1000
	target_update_freq: 4
	gaussian_policy: False
	grad_clip_eps: 0.5
	agent_num: 4
	obs_dims: [2, 2, 2, 2]
	action_dim: 1
env_params:
	A: 1.0
	rho_a: 1.0
	rho_b: 1.0
	rho_w: 1.0
	r: 1.0
	ko: 1.0
	E: 1.0
	R: 1.0
	cp_w: 1.0
	cp_a: 1.0
	cp_b: 1.0
	dHr: 1.0
	Tref: 1.0
	Cai: 1.0
	Cbi: 1.0
	h_sp: 3.0
	Ca_sp: 1.0
	Cb_sp: 1.0
	Cc_sp: 50.0
	T_sp: 320.0
	h_min: 1.0
	h_max: 1.0
	T_min: 298.0
	T_max: 450.0
	dt: 1.0
	max_steps: 30
	initial_state: [1.0, 1.0, 1.0, 1.0, 1.0]
	reward_type: normal 
